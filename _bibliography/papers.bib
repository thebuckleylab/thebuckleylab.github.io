---
---

@misc{collis2024learninghybridactiveinference,
      title={Learning in Hybrid Active Inference Models}, 
      author={Poppy Collis and Ryan Singh and Paul F Kinghorn and Christopher L Buckley},
      year={2024},
      eprint={2409.01066},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2409.01066}, 
}


@misc{innocenti2024strictsaddlesenergylandscape,
      title={Only Strict Saddles in the Energy Landscape of Predictive Coding Networks?}, 
      author={Francesco Innocenti and El Mehdi Achour and Ryan Singh and Christopher L. Buckley},
      year={2024},
      eprint={2408.11979},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2408.11979}, 
      selected={true}
}


@inproceedings{collis2024hybrid,
title={Hybrid Recurrent Models Support Emergent Descriptions for Hierarchical Planning and Control},
author={Poppy Collis and Ryan Singh and Paul Kinghorn and Christopher Buckley},
booktitle={ICML 2024 Workshop: Foundations of Reinforcement Learning and Control -- Connections and Perspectives},
year={2024},
url={https://arxiv.org/abs/2408.10970}
}

@InProceedings{10.1007/978-3-031-47958-8_14,
abbr={CCIS},
author="Kiefer, Alex B.
and Buckley, Christopher L.",
editor="Buckley, Christopher L.
and Cialfi, Daniela
and Lanillos, Pablo
and Ramstead, Maxwell
and Sajid, Noor
and Shimazaki, Hideaki
and Verbelen, Tim
and Wisse, Martijn",
abstract="Although the latent spaces learned by distinct neural networks are not generally directly comparable, even when model architecture and training data are held fixed, recent work in machine learning [13] has shown that it is possible to use the similarities and differences among latent space vectors to derive ``relative representations'' with comparable representational power to their ``absolute'' counterparts, and which are nearly identical across models trained on similar data distributions. Apart from their intrinsic interest in revealing the underlying structure of learned latent spaces, relative representations are useful to compare representations across networks as a generic proxy for convergence, and for zero-shot model stitching [13].",
title="Relative Representations for Cognitive Graphs",
booktitle="Active Inference",
year="2024",
publisher="Springer Nature Switzerland",
address="Cham",
pages="218--236",
isbn="978-3-031-47958-8"
}


@InProceedings{10.1007/978-3-031-47958-8_12,
abbr={CCIS},
author="De Llanza Varona, Miguel
and Buckley, Christopher
and Millidge, Beren",
editor="Buckley, Christopher L.
and Cialfi, Daniela
and Lanillos, Pablo
and Ramstead, Maxwell
and Sajid, Noor
and Shimazaki, Hideaki
and Verbelen, Tim
and Wisse, Martijn",
abstract="Organisms have to keep track of the information in the environment that is relevant for adaptive behaviour. Transmitting information in an economical and efficient way becomes crucial for limited-resourced agents living in high-dimensional environments. The efficient coding hypothesis claims that organisms seek to maximize the information about the sensory input in an efficient manner. Under Bayesian inference, this means that the role of the brain is to efficiently allocate resources in order to make predictions about the hidden states that cause sensory data. However, neither of those frameworks accounts for how that information is exploited downstream, leaving aside the action-oriented role of the perceptual system. Rate-distortion theory, which defines optimal lossy compression under constraints, has gained attention as a formal framework to explore goal-oriented efficient coding. In this work, we explore action-centric representations in the context of rate-distortion theory. We also provide a mathematical definition of abstractions and we argue that, as a summary of the relevant details, they can be used to fix the content of action-centric representations. We model action-centric representations using VAEs and we find that such representations i) are efficient lossy compressions of the data; ii) capture the task-dependent invariances necessary to achieve successful behaviour; and iii) are not in service of reconstructing the data. Thus, we conclude that full reconstruction of the data is rarely needed to achieve optimal behaviour, consistent with a teleological approach to perception.",
title="Exploring Action-Centric Representations Through the Lens of Rate-Distortion Theory",
booktitle="Active Inference",
year="2024",
publisher="Springer Nature Switzerland",
address="Cham",
pages="189--203",
isbn="978-3-031-47958-8"
}

}

@InProceedings{10.1007/978-3-031-47958-8_4,
abbr={CCIS},
author="Collis, Poppy
and Kinghorn, Paul F.
and Buckley, Christopher L.",
editor="Buckley, Christopher L.
and Cialfi, Daniela
and Lanillos, Pablo
and Ramstead, Maxwell
and Sajid, Noor
and Shimazaki, Hideaki
and Verbelen, Tim
and Wisse, Martijn",
abstract="The ability to invent new tools has been identified as an important facet of our ability as a species to problem solve in dynamic and novel environments [17]. While the use of tools by artificial agents presents a challenging task and has been widely identified as a key goal in the field of autonomous robotics, far less research has tackled the invention of new tools by agents. In this paper, (1) we articulate the distinction between tool discovery and tool innovation by providing a minimal description of the two concepts under the formalism of active inference. We then (2) apply this description to construct a toy model of tool innovation by introducing the notion of tool affordances into the hidden states of the agent's probabilistic generative model. This particular state factorisation facilitates the ability to not just discover tools but invent them through the offline induction of an appropriate tool property. We discuss the implications of these preliminary results and outline future directions of research.",
title="Understanding Tool Discovery and Tool Innovation Using Active Inference",
booktitle="Active Inference",
year="2024",
publisher="Springer Nature Switzerland",
address="Cham",
pages="43--58",
isbn="978-3-031-47958-8"
}

@misc{salvatori2023braininspired,
  abbr={arXiv},
  title={Brain-Inspired Computational Intelligence via Predictive Coding}, 
  author={Tommaso Salvatori and Ankur Mali and Christopher L. Buckley and Thomas Lukasiewicz and Rajesh P. N. Rao and Karl Friston and Alexander Ororbia},
  year={2023},
  eprint={2308.07870},
  archivePrefix={arXiv},
  primaryClass={cs.AI},
}

@inproceedings{singh2023attention,
abbr={ICML},
title={Attention as Implicit Structural Inference},
author={Ryan Singh and Christopher Buckley},
booktitle={ICML 2023 Workshop on Structured Probabilistic Inference {\&} Generative Modeling},
year={2023},
url={https://openreview.net/forum?id=v9EOxGrpwy}
}

@inproceedings{innocenti2023understanding,
abbr={ICML},
title={Understanding Predictive Coding as a Second-Order Trust-Region Method},
author={Francesco Innocenti and Ryan Singh and Christopher Buckley},
booktitle={ICML Workshop on Localized Learning (LLW)},
year={2023},
url={https://openreview.net/forum?id=x7PUpFKZ8M}
}

@misc{singh2023attention,
  abbr={arXiv},
  title={Attention: Marginal Probability is All You Need?}, 
  author={Ryan Singh and Christopher L. Buckley},
  year={2023},
  eprint={2304.04556},
  archivePrefix={arXiv},
  primaryClass={cs.LG}
}

@article{AGUILERA2023270,
  abbr={PoL},
  title={From the free energy principle to a confederation of Bayesian mechanics&#58; Reply to comments on “How particular is the physics of the free energy principle?”},
  author={Miguel Aguilera and Beren Millidge and Alexander Tschantz and Christopher L. Buckley},
  journal={Physics of Life Reviews},
  volume = {44},
  pages={270-275},
  year={2023},
  issn = {1571-0645},
  doi={https://doi.org/10.1016/j.plrev.2023.01.018},
  url={https://www.sciencedirect.com/science/article/pii/S1571064523000192},
}

@article{Holman2023,
  abbr={PLoS Comp. Bio.},
  title = {A behavioral and modeling study of control algorithms underlying the translational optomotor response in larval zebrafish with implications for neural circuit function},
  author = {John G Holman and Winnie W K Lai and Paul Pichler and Daniel Saska and Leon Lagnado and Christopher Buckley},
  journal={PLoS Computational Biology},
  volume={19},
  pages = {1-35},
  year = {2023},
  doi = {10.1371/journal.pcbi.1010924},
  url = {https://doi.org/10.1371/journal.pcbi.1010924},
}

@InProceedings{10.1007/978-3-031-28719-0_22,
abbr={CCIS},
author="Koudahl, Magnus
and Buckley, Christopher L.
and de Vries, Bert",
editor="Buckley, Christopher L.
and Cialfi, Daniela
and Lanillos, Pablo
and Ramstead, Maxwell
and Sajid, Noor
and Shimazaki, Hideaki
and Verbelen, Tim",
title="A Message Passing Perspective on Planning Under Active Inference",
booktitle="Active Inference",
year="2023",
publisher="Springer Nature Switzerland",
address="Cham",
pages="319--327",
isbn="978-3-031-28719-0"
}

@InProceedings{10.1007/978-3-031-28719-0_11,
abbr="CCIS",
author="Millidge, Beren
and Buckley, Christopher L.",
editor="Buckley, Christopher L.
and Cialfi, Daniela
and Lanillos, Pablo
and Ramstead, Maxwell
and Sajid, Noor
and Shimazaki, Hideaki
and Verbelen, Tim",
title="Active Inference Successor Representations",
booktitle="Active Inference",
year="2023",
publisher="Springer Nature Switzerland",
address="Cham",
pages="151--161",
isbn="978-3-031-28719-0"
}

@InProceedings{10.1007/978-3-031-28719-0_14,
abbr="CCIS",
author="Kiefer, Alex B.
and Millidge, Beren
and Tschantz, Alexander
and Buckley, Christopher L.",
editor="Buckley, Christopher L.
and Cialfi, Daniela
and Lanillos, Pablo
and Ramstead, Maxwell
and Sajid, Noor
and Shimazaki, Hideaki
and Verbelen, Tim",
title="Capsule Networks as Generative Models",
booktitle="Active Inference",
year="2023",
publisher="Springer Nature Switzerland",
address="Cham",
pages="192--209",
isbn="978-3-031-28719-0"
}

@InProceedings{10.1007/978-3-031-28719-0_5,
abbr="CCIS",
author="Aguilera, Miguel
and Poc-L{\'o}pez, {\'A}ngel
and Heins, Conor
and Buckley, Christopher L.",
editor="Buckley, Christopher L.
and Cialfi, Daniela
and Lanillos, Pablo
and Ramstead, Maxwell
and Sajid, Noor
and Shimazaki, Hideaki
and Verbelen, Tim",
title="Knitting a Markov Blanket is Hard When You are Out-of-Equilibrium&#58; Two Examples in Canonical Nonequilibrium Models",
booktitle="Active Inference",
year="2023",
publisher="Springer Nature Switzerland",
address="Cham",
pages="65--74",
isbn="978-3-031-28719-0"
}


@InProceedings{10.1007/978-3-031-28719-0_1,
abbr="CCIS",
author="Kinghorn, Paul F.
and Millidge, Beren
and Buckley, Christopher L.",
editor="Buckley, Christopher L.
and Cialfi, Daniela
and Lanillos, Pablo
and Ramstead, Maxwell
and Sajid, Noor
and Shimazaki, Hideaki
and Verbelen, Tim",
title="Preventing Deterioration of Classification Accuracy in Predictive Coding Networks",
booktitle="Active Inference",
year="2023",
publisher="Springer Nature Switzerland",
address="Cham",
pages="1--15",
isbn="978-3-031-28719-0"
}

@InProceedings{10.1007/978-3-031-28719-0_6,
abbr="CCIS",
author="Heins, Conor
and Klein, Brennan
and Demekas, Daphne
and Aguilera, Miguel
and Buckley, Christopher L.",
editor="Buckley, Christopher L.
and Cialfi, Daniela
and Lanillos, Pablo
and Ramstead, Maxwell
and Sajid, Noor
and Shimazaki, Hideaki
and Verbelen, Tim",
title="Spin Glass Systems as Collective Active Inference",
booktitle="Active Inference",
year="2023",
publisher="Springer Nature Switzerland",
address="Cham",
pages="75--98",
isbn="978-3-031-28719-0"
}

@article{10.1162/neco_a_01497,
    abbr={Neural Comput.},
    author = {Millidge, Beren and Tschantz, Alexander and Buckley, Christopher L.},
    title = "{Predictive Coding Approximates Backprop Along Arbitrary Computation Graphs}",
    journal = {Neural Computation},
    volume = {34},
    number = {6},
    pages = {1329-1368},
    year = {2022},
    month = {05},
    issn = {0899-7667},
    doi = {10.1162/neco_a_01497},
    url = {https://doi.org/10.1162/neco\_a\_01497},
    eprint = {https://direct.mit.edu/neco/article-pdf/34/6/1329/2023477/neco\_a\_01497.pdf},
}

@misc{tschantz2022hybrid,
  abbr={arXiv},
  title={Hybrid Predictive Coding&#58; Inferring, Fast and Slow},
  author={Alexander Tschantz and Beren Millidge and Anil K Seth and Christopher L Buckley},
  year={2022},
  eprint={2204.02169},
  archivePrefix={arXiv},
  primaryClass={q-bio.NC}
}
